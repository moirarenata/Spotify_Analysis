{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json \n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from os import path\n",
    "import string\n",
    "\n",
    "import nltk \n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read JSON file from Genius API scraping\n",
    "df = json.load(open(\"data/Lyrics_NoahKahan.json\", \"r\"))\n",
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df['songs'])\n",
    "df['release_date']=pd.to_datetime(df['release_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean=df[['title','lyrics','release_date']]\n",
    "\n",
    "#adding album name into the dataframe\n",
    "\n",
    "album_list=[]\n",
    "for i in range(len(df['album'])):\n",
    "    try:\n",
    "        album_list.append(df['album'][i]['name'])\n",
    "    except:\n",
    "        album_list.append(None)\n",
    "        continue\n",
    "df_clean['Album']=album_list\n",
    "\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Lyrics\n",
    "\n",
    "1. Remove the first few phrases before the actual lyrics \n",
    "2. Remove the ones that says 'lyrics from instagram live, lyrics from live performance' etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_lyrics(text):\n",
    "    # Keep only lyrics (after \"Lyrics\")\n",
    "    match = re.search(r'Lyrics\\s*(.*)', text, re.DOTALL)\n",
    "    text = match.group(1).strip() if match else text  \n",
    "\n",
    "    # Remove unwanted sources (Live, Instagram, TikTok, etc.)\n",
    "    text = re.sub(r'(?i)Lyrics from .*?Live.*?\\n+', '', text)  \n",
    "    text = re.sub(r'(?i)Lyrics from Instagram.*?\\n+', '', text)  \n",
    "    text = re.sub(r'(?i)Lyrics From TikTok Video.*?\\n+', '', text) \n",
    "    text = re.sub(r'(?i)Lyrics from Youtube Short.*?\\n+', '', text) \n",
    "    text = re.sub(r'(?i)Lyrics from X & TikTok Video.*?\\n+', '', text) \n",
    "\n",
    "    return text.strip()  \n",
    "\n",
    "def clean_song_title(song):\n",
    "    \"\"\"Removes version details from song titles (e.g., 'Live', 'Acoustic').\"\"\"\n",
    "    return re.sub(r\"\\s*\\(.*?\\)\", \"\", song).strip()\n",
    "\n",
    "df_clean[\"lyrics\"] = df_clean[\"lyrics\"].apply(clean_lyrics)\n",
    "df_clean[\"title\"] = df_clean[\"title\"].apply(clean_song_title)\n",
    "df_clean = df_clean.drop_duplicates(subset=[\"title\"], keep=\"first\")\n",
    "\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Lyrics for NLTK Analysis\n",
    "\n",
    "1. Convert to lowercase\n",
    "2. Remove text inside square brackets (e.g. [Verse 1])\n",
    "3. Remove newlines \n",
    "4. Remove punctuation, numbers, URLs\n",
    "5. Remove stopwards as defined by ntlk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()  \n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  \n",
    "    text = re.sub(r'\\n', ' ', text)  \n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation)) \n",
    "    text = re.sub(r'\\d+', '', text)  \n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)  \n",
    "    words = text.split() \n",
    "    words = [word for word in words if word not in stop_words] \n",
    "    return \" \".join(words)\n",
    "\n",
    "df_clean[\"cleaned_lyrics\"] = df_clean[\"lyrics\"].apply(clean_text)\n",
    "df_clean = df_clean.drop(['lyrics'], axis = 1)\n",
    "\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words(\"english\"))\n",
    "\n",
    "#Mapping POS tag to first character lemmatize() accepts\n",
    "def get_wordnet_pos(word):\n",
    "    \n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "#Initializing Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#Function to Lemmatize every word and remove stopwords \n",
    "def lemma(text):\n",
    "    text = [lemmatizer.lemmatize(x, get_wordnet_pos(x)) for x in nltk.word_tokenize(text)]\n",
    "    text = [x for x in text if x not in stop]\n",
    "    return ' '.join(text)\n",
    "\n",
    "df_clean['cleaned_lyrics'] = df_clean['cleaned_lyrics'].apply(lambda x: lemma(x))\n",
    "df_clean.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['WordLength'] = np.array([len(text) for text in df_clean['cleaned_lyrics']])\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.to_csv(\"data/Lyrics_Noah_Kahan.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Lyrics for NLTK Analysis\n",
    "\n",
    "1. Remove text inside square brackets (e.g. [Verse 1])\n",
    "2. Remove newlines and replace with ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_lyrics(text):\n",
    "    text = re.sub(r\"\\[.*?\\]\", \"\", text)  \n",
    "    text = text.replace(\"\\n\", \". \")  \n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  \n",
    "    return text\n",
    "\n",
    "df_clean[\"cleaned_lyrics_2\"] = df_clean[\"lyrics\"].apply(clean_lyrics)\n",
    "df_clean.to_csv(\"data/Lyrics_Noah_Kahan_bert.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
